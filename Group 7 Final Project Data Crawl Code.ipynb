{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c53879d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tag page data crawl code\n",
    "import requests\n",
    "import csv\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "def scrape_xiaohongshu(base_url):\n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/86.0.4240.198 Safari/537.36'\n",
    "    }\n",
    "\n",
    "    with open('Xiaohongshu Tag Page Data.csv', mode='w', newline='', encoding='utf-8') as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerow(['Note ID', 'Title', 'Likes', 'User Nickname', 'User ID', 'Image URL', 'Create Time'])\n",
    "\n",
    "        cursor = ''\n",
    "        while True:\n",
    "            url = f\"{base_url}&cursor={cursor}\" if cursor else base_url\n",
    "            response = requests.get(url, headers=headers)\n",
    "            try:\n",
    "                data = response.json()\n",
    "            except json.JSONDecodeError:\n",
    "                print(\"Unable to parse JSON: \", response.text)\n",
    "                break\n",
    "\n",
    "            if not data['data']['notes']:\n",
    "                break\n",
    "\n",
    "            for note in data['data']['notes']:\n",
    "                note_id = note['id']\n",
    "                title = note['title']\n",
    "                likes = note['likes']\n",
    "                user_nickname = note['user']['nickname']\n",
    "                user_id = note['user']['userid']\n",
    "                image_url = note['images_list'][0]['url'] if note['images_list'] else ''\n",
    "                create_time_ms = note['create_time']\n",
    "                create_time = datetime.fromtimestamp(create_time_ms / 1000).strftime('%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "                writer.writerow([note_id, title, likes, user_nickname, user_id, image_url, create_time])\n",
    "\n",
    "            cursor = data['data']['notes'][-1]['cursor']\n",
    "\n",
    "base_url = 'https://www.xiaohongshu.com/web_api/sns/v3/page/notes?page_size=6&sort=hot&page_id=64fe7e93e0687e0001d2dcb3'\n",
    "scrape_xiaohongshu(base_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f51a222",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specific notes comment data crawl code\n",
    "import requests\n",
    "import csv\n",
    "\n",
    "def scrape_comments(note_id, initial_cursor=''):\n",
    "    base_url = \"https://edith.xiaohongshu.com/api/sns/web/v2/comment/page\"\n",
    "    params = {\n",
    "        'note_id': note_id,\n",
    "        'cursor': initial_cursor,\n",
    "        'image_formats': 'jpg,webp,avif'\n",
    "    }\n",
    "    headers = {\n",
    "       'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.36',\n",
    "            'Cookie': 'abRequestId=0c3c6cb4-55a3-5fe1-9e83-b8256aae2d84; a1=18b4ff17de8wc6dr7viwf7ow22xwvs8s8j1yb1sng30000326940; webId=68369f8f2b369f65ba484733d2749b18; gid=yYD4iiyYJKY0yYD4iiyWfVyMdYESKfFWh3EiyIkWlEJJCvq8EhMYMD888qJKj48804DKD8Y4; unread={%22ub%22:%22656bf955000000000602378a%22%2C%22ue%22:%2265727c500000000038023457%22%2C%22uc%22:23}; xsecappid=xhs-pc-web; web_session=040069b4473273f90a70bb824f374b332cda18; websectiga=2a3d3ea002e7d92b5c9743590ebd24010cf3710ff3af8029153751e41a6af4a3; sec_poison_id=35fcb4c8-c29d-4482-a53e-001baa796971; webBuild=3.20.2'\n",
    "    }\n",
    "\n",
    "    processed_comments = set() \n",
    "\n",
    "    with open('Xiaohongshu Note Comments Data.csv', mode='w', newline='', encoding='utf-8') as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerow(['User ID', 'Nickname', 'Content', 'Like Count', 'IP Location'])\n",
    "\n",
    "        while True:\n",
    "            response = requests.get(base_url, params=params, headers=headers)\n",
    "            data = response.json()\n",
    "\n",
    "            if not data['data']['comments']:\n",
    "                break\n",
    "\n",
    "            process_comments(data['data']['comments'], writer, note_id, processed_comments)\n",
    "\n",
    "            params['cursor'] = data['data']['cursor']\n",
    "\n",
    "def process_comments(comments, writer, note_id, processed_comments):\n",
    "    for comment in comments:\n",
    "        if comment['id'] not in processed_comments:\n",
    "            write_comment(comment, writer)\n",
    "            processed_comments.add(comment['id'])\n",
    "\n",
    "            if 'sub_comments' in comment and comment['sub_comments']:\n",
    "                process_comments(comment['sub_comments'], writer, note_id, processed_comments)\n",
    "        \n",
    "            if comment.get('sub_comment_has_more'):\n",
    "                fetch_and_process_additional_sub_comments(note_id, comment['id'], writer, processed_comments)\n",
    "\n",
    "def write_comment(comment, writer):\n",
    "    user_id = comment['user_info']['user_id']\n",
    "    nickname = comment['user_info']['nickname']\n",
    "    content = comment['content']\n",
    "    like_count = comment.get('like_count', 0)\n",
    "    ip_location = comment.get('ip_location', '')\n",
    "    writer.writerow([user_id, nickname, content, like_count, ip_location])\n",
    "\n",
    "def fetch_and_process_additional_sub_comments(note_id, root_comment_id, writer, processed_comments):\n",
    "    sub_comments_url = \"https://edith.xiaohongshu.com/api/sns/web/v2/comment/sub/page\"\n",
    "    cursor = \"\"\n",
    "\n",
    "    while True:\n",
    "        params = {\n",
    "            'note_id': note_id,\n",
    "            'root_comment_id': root_comment_id,\n",
    "            'num': 10,\n",
    "            'cursor': cursor,\n",
    "            'image_formats': 'jpg,webp,avif'\n",
    "        }\n",
    "        headers = {\n",
    "            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.36',\n",
    "            'Cookie': 'abRequestId=0c3c6cb4-55a3-5fe1-9e83-b8256aae2d84; a1=18b4ff17de8wc6dr7viwf7ow22xwvs8s8j1yb1sng30000326940; webId=68369f8f2b369f65ba484733d2749b18; gid=yYD4iiyYJKY0yYD4iiyWfVyMdYESKfFWh3EiyIkWlEJJCvq8EhMYMD888qJKj48804DKD8Y4; unread={%22ub%22:%22656bf955000000000602378a%22%2C%22ue%22:%2265727c500000000038023457%22%2C%22uc%22:23}; xsecappid=xhs-pc-web; web_session=040069b4473273f90a70bb824f374b332cda18; websectiga=2a3d3ea002e7d92b5c9743590ebd24010cf3710ff3af8029153751e41a6af4a3; sec_poison_id=35fcb4c8-c29d-4482-a53e-001baa796971; webBuild=3.20.2'\n",
    "        }\n",
    "\n",
    "        response = requests.get(sub_comments_url, params=params, headers=headers)\n",
    "        data = response.json()\n",
    "\n",
    "        if not data['data']['comments']:\n",
    "            break\n",
    "\n",
    "        process_comments(data['data']['comments'], writer, note_id, processed_comments)\n",
    "\n",
    "        cursor = data['data'].get('cursor')\n",
    "        if not cursor:\n",
    "            break\n",
    "\n",
    "note_id = '655f4fd4000000001b00dafd' \n",
    "scrape_comments(note_id)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
